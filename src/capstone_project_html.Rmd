---
title: "Analysing Dengue Cases in Singapore"
author: "Shayne, Roscoe, Anu"
date: "02 August 2020"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "",
  echo = T,
  fig.width = 9,
  warning = F
)

# Set the working directory to this file's location (for RStudio)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

# Analysing Dengue Cases in Singapore

## Problem Statement

In our Capstone Project, we aim to explore the relationship between meteorological conditions (Air temperature and rainfall) of Singapore and clinical conditions commonly seen in local community â€“ Dengue fever (DF), hand food mouth disease (HFMD), upper respiratory tract infection (URTI) and diarrhea. Specifically, does higher temperature and rainfall associated with the rise in clinical conditions in the community? Furthermore, we are interested to investigate if types of housing is associated with the number of Dengue fever cases.

Dengue fever is a vector-borne infectious disease that is endemic in the tropical world. Singapore is one of several countries with high disease burden of dengue. In 2020, Singapore saw 1,158 dengue cases in a week of June - the highest number of weekly dengue cases ever recorded since 2014. The sudden increase is worrisome, therefore greater emphasis on analysis will be focused on Dengue fever.

## Activate packages

```{r load-packages}
# Rule for activating packages in development mode:
# - Specify everything fully
# - Exceptions:
#   - ggfortify - to overload `ggplot2::autoplot()`
#   - ggmap - has to be activated for `ggmap::register_google()` to work
#   - ggplot2 - too troublesome otherwise
#   - magrittr - `%>%`
#
# For deployment mode, activate every package that is referenced more than once
# - Add package name here, then use find-and-replace to remove "<package>::"
# - Allowed to activate tidyverse
#   - Remember what are the 8 packages?

pacman::p_load(
  ggfortify,
  # ggmap,
  ggplot2,
  magrittr
  # tidyverse
)
```

## Import

> Most time-consuming, finding data, is.

All data are available online and first gathered into separate distinct raw datasets:

* Meteorological Service Singapore (MSS)
  + [Historical daily weather records](http://www.weather.gov.sg/climate-historical-daily/)
  + [List of stations, weather parameters and periods of records available](http://www.weather.gov.sg/wp-content/uploads/2016/12/Station_Records.pdf)
* Ministry of Health (MOH)
  + [Weekly infectious disease bulletin](https://www.moh.gov.sg/resources-statistics/infectious-disease-statistics/2020/weekly-infectious-diseases-bulletin)
  + [Healthcare Institutions (HCI) Directory](http://hcidirectory.sg/hcidirectory/)
* [Data.gov.sg](https://data.gov.sg/)
  + [Dengue cases](https://data.gov.sg/search?q=denguecases)
  + [_Aedes_ mosquito breeding habitats](https://data.gov.sg/search?q=aedes+habitats)
  + [Singapore residents by planning area and type of dwelling, Jun 2017](https://data.gov.sg/dataset/singapore-residents-by-planning-area-and-type-of-dwelling-jun-2017)
  + [Master Plan 2014 Region Boundary (Web)](https://data.gov.sg/dataset/master-plan-2014-region-boundary-web)

[Click here](https://github.com/roscoelai/dasr2020capstone#data) for a summary page. The full glorious details of their providence are given below.

#### Helper functions

> The following functions **must** be activated no matter which method of data import is chosen.

```{r import-helper-functions}
import_moh_weekly <- function(url_or_path = NULL) {
  #' Weekly Infectious Diseases Bulletin
  #' 
  #' @description
  #' \href{https://www.moh.gov.sg/resources-statistics/infectious-disease-
  #' statistics/2020/weekly-infectious-diseases-bulletin}{MOH Weekly Infectious 
  #' Disease Bulletin} from the Ministry of Health (MOH). 
  #' 
  #' \href{https://www.moh.gov.sg/docs/librariesprovider5/diseases-updates/
  #' weekly-infectious-disease-bulletin-year-2020
  #' d1092fcb484447bc96ef1722b16b0c08.xlsx}{Latest data as of 31 July 2020 
  #' (2012-W01 to 2020-W30)}.
  #' 
  #' @param url_or_path The URL or file path of the .xlsx file.
  #' @return Weekly infectious diseases bulletin (2012-W01 to 2020-W30).
  
  # If no URL or path is specified, try to get the file directly from MOH.
  if (is.null(url_or_path)) {
    url_or_path = paste0(
      "https://www.moh.gov.sg/docs/librariesprovider5/diseases-updates/",
      "weekly-infectious-disease-bulletin-year-2020",
      "d1092fcb484447bc96ef1722b16b0c08.xlsx"
    )
  }
  
  # Check if the given path is a URL by trying to download to a temp file. If 
  #   successful, return the temp file. If not, return the original path.
  xlsx_file = tryCatch({
    temp = tempfile(fileext = ".xlsx")
    download.file(url_or_path, destfile = temp, mode = "wb")
    temp
  }, error = function(e) {
    url_or_path
  })
  
  # Columns will be renamed to follow 2020
  colnames_2020 = c(
    "Campylobacter enterosis" = "Campylobacter enteritis",
    "Campylobacterenterosis" = "Campylobacter enteritis",
    "Campylobacteriosis" = "Campylobacter enteritis",
    "Chikungunya Fever" = "Chikungunya",
    "Dengue Haemorrhagic Fever" = "DHF",
    "Dengue Fever" = "Dengue",
    "Hand, Foot and Mouth Disease" = "HFMD",
    "Hand, Foot Mouth Disease" = "HFMD",
    "Nipah virus infection" = "Nipah",
    "Viral Hepatitis A" = "Acute Viral Hepatitis A",
    "Viral Hepatitis E" = "Acute Viral Hepatitis E",
    "Zika Virus Infection" = "Zika",
    "Zika virus infection" = "Zika"
  )
  
  xlsx_file %>%
    readxl::excel_sheets() %>% 
    lapply(function(sheetname) {
      df = readxl::read_xlsx(xlsx_file, sheetname, skip = 1)
      
      # Date formats are different for 2020 (dmy instead of mdy)
      if (sheetname == "2020") {
        df$Start = lubridate::dmy(df$Start)
        df$End = lubridate::dmy(df$End)
      }
      
      # Find and rename columns that need to be renamed, and rename them
      mapper = na.omit(colnames_2020[names(df)])
      dplyr::rename_with(df, ~mapper, names(mapper))
    }) %>% 
    dplyr::bind_rows() %>% 
    dplyr::rename(Epiweek = `Epidemiology Wk`) %>% 
    dplyr::mutate(Epiyear = lubridate::epiyear(Start)) %>% 
    dplyr::select(Epiyear, everything()) %>% 
    dplyr::arrange(Start)
}

read_kmls <- function(url_or_path) {
  #' Read Single or Multiple KML Files
  #' 
  #' @description
  #' There are (at least) 2 approaches to handling .kml data:
  #' \enumerate{
  #'   \item sp - rgdal::readOGR()
  #'   \item sf - sf::st_read()
  #' }
  #'
  #' The sp approach was abandoned as their objects were rather complex and 
  #'   did not facilitate method chaining.
  #'
  #' The sf approach produced objects that look like data.frames, which had 
  #'   better support for method chaining, but also some peculiarities:
  #' \itemize{
  #'   \item Dimension: set to XY using sf::st_zm()
  #'   \item Geographic CRS: use sf::`st_crs<-`("WGS84") World Geodetic 
  #'         Survey 1984
  #' }
  #' 
  #' @param url_or_path The URL(s) or file path(s) of the .kml file(s).
  #' @return A single combined sf object.
  
  # Check if the given paths are URLs by trying to download to temp files. If 
  #   successful, return the temp files. If not, return the original paths. 
  #   Automatically extract .zip files, if any.
  kml_files = tryCatch({
    temp = tempfile(fileext = paste0(".", tools::file_ext(url_or_path)))
    Map(function(x, y) download.file(x, y, mode = "wb"), url_or_path, temp)
    sapply(temp, function(x) {
      if (endsWith(x, ".zip")) {
        unzip(x)
      } else {
        x
      }
    })
  }, error = function(e) {
    url_or_path
  })
  
  kml_files %>% 
    lapply(sf::st_read, quiet = T) %>% 
    dplyr::bind_rows() %>% 
    tibble::as_tibble() %>% 
    sf::st_as_sf() %>% 
    sf::st_zm()
}
```

### Import data _de novo_ (where available)

The following code chunk has been disabled, but is provided here to show the details behind the webscraping processes.

```{r import-webscraping, eval=FALSE}
import_mss_daily <- function(years, stations = NULL) {
  #' Historical Daily Weather Records
  #' 
  #' @description
  #' \href{http://www.weather.gov.sg/climate-historical-daily/}{Daily weather 
  #' records} from the Meteorological Service Singapore (MSS). 
  #' Available data ranges from January 1980 to June 2020. Only 19 of the 63 
  #' climate stations are recognized by this function, because these contain 
  #' more than just rainfall data. \href{http://www.weather.gov.sg/wp-content/
  #' uploads/2016/12/Station_Records.pdf}{List of stations, weather parameters 
  #' and periods of records}.
  #' 
  #' MSS is nice enough to have their data in .csv files, with a systematic 
  #' naming scheme. Data compilation becomes as simple as generating the 
  #' appropriate list of URLs.
  #' 
  #' @param years A vector of the years of interest.
  #' @param stations A vector of the climate station names.
  #' @return Combined daily weather records.
  #' @examples
  #' import_mss_daily(2012:2020, "Changi")
  #' import_mss_daily(2012:2020, c("Changi", "Clementi", "Khatib", "Newton"))
  
  stations_lookup = c(
    "Admiralty" = "104_",
    "Ang Mo Kio" = "109_",
    "Changi" = "24_",
    "Choa Chu Kang (South)" = "121_",
    "Clementi" = "50_",
    "East Coast Parkway" = "107_",
    "Jurong (West)" = "44_",
    "Jurong Island" = "117_",
    "Khatib" = "122_",
    "Marina Barrage" = "108_",
    "Newton" = "111_",
    "Pasir Panjang" = "116_",
    "Pulau Ubin" = "106_",
    "Seletar" = "25_",
    "Sembawang" = "80_",
    "Sentosa Island" = "60_",
    "Tai Seng" = "43_",
    "Tengah" = "23_",
    "Tuas South" = "115_"
  )
  
  # Check that all provided station names are in the list, if not, exit and 
  #   print out the list (of names) for the user.
  mask = !(stations %in% names(stations_lookup))
  if (any(mask)) {
    stop("The following station names are not recognized:\n",
         paste(stations[mask], collapse = "\n"),
         "\n\nPlease select from the following:\n",
         paste(names(stations_lookup), collapse = "\n"))
  }
  
  # If no station names specified, take the full list
  if (is.null(stations)) {
    station_nums = stations_lookup
  } else {
    station_nums = stations_lookup[stations]
  }
  
  # The URL to each .csv file has the following format:
  # - "<url_base><station number>_<YYYY><MM>.csv"
  # - Notice the station numbers given above contain the "_" suffix
  url_base = "http://www.weather.gov.sg/files/dailydata/DAILYDATA_S"
  
  # To enumerate all the URLs, take the Cartesian product of:
  # - station numbers
  # - years
  # - months (we'll always attempt to find all 12 months)
  #
  # Flatten the result into a vector, then prefix and suffix with `url_base` 
  #   and ".csv", respectively.
  
  # Base R
  urls = station_nums %>% 
    outer(years, FUN = paste0) %>% 
    outer(sprintf("%02d", 1:12), FUN = paste0) %>% 
    sort() %>%  # Flatten and arrange in alphabetical order
    paste0(url_base, ., ".csv")
  
  # Equivalent tidyverse approach (for reference)
  # urls = station_nums %>% 
  #   tidyr::crossing(years, sprintf("%02d", 1:12)) %>% 
  #   tidyr::unite("station_year_month", sep = "") %>% 
  #   dplyr::pull() %>% 
  #   paste0(url_base, ., ".csv")
  
  # We have a list of URLs, but some of them may not exist (some stations do 
  #   not have data for some months). Use tryCatch() to return a table for the 
  #   URLs that exist, and a NA for those that don't.
  #
  # Problems with multibyte characters and countermeasures:
  # - Some colnames contained the "degree sign" symbol which somehow made the 
  #   table contents inaccessible. Tables after April 2020 had slightly 
  #   different colnames.
  #   - Manually set the colnames for each table.
  # - Some tables contained the "em dash" symbol to indicate missing values.
  #   - They will be coerced to NA when the columns are type cast to numeric.
  #   - There will be warning messages.
  dfs = urls %>% 
    lapply(function(url) {
      tryCatch({
        df = readr::read_csv(url,
                             skip = 1,
                             col_names = c(
                               "Station",
                               "Year",
                               "Month",
                               "Day",
                               "Rainfall",
                               "Highest_30_min_rainfall",
                               "Highest_60_min_rainfall",
                               "Highest_120_min_rainfall",
                               "Mean_temp",
                               "Max_temp",
                               "Min_temp",
                               "Mean_wind",
                               "Max_wind"
                             ),
                             col_types = readr::cols_only(
                               "Station" = "c",
                               "Year" = "n",
                               "Month" = "n",
                               "Day" = "n",
                               "Rainfall" = "n",
                               "Mean_temp" = "n",
                               "Max_temp" = "n",
                               "Min_temp" = "n"
                             ))
        
        # Announce progress (UX is important! We can tolerate lower efficiency)
        message(paste(df[1, 1:3], collapse = "_"))
        
        df
      }, error = function(e) {
        NA
      })
    })
  
  dfs[!is.na(dfs)] %>% 
    dplyr::bind_rows() %>% 
    # Calculate daily temperature range
    dplyr::mutate(Temp_range = Max_temp - Min_temp,
                  .keep = "unused") %>% 
    # Calculate epidemiological years and weeks
    dplyr::mutate(Date = lubridate::ymd(paste(Year, Month, Day, sep = "-")),
                  Epiyear = lubridate::epiyear(Date),
                  Epiweek = lubridate::epiweek(Date),
                  .keep = "unused",
                  .after = Station) %>% 
    dplyr::arrange(Station, Date)
}

import_hcidirectory <- function() {
  #' Healthcare Institutions Directory
  #' 
  #' @description
  #' The \href{http://hcidirectory.sg/hcidirectory/}{Healthcare Institutions 
  #' (HCI) Directory}, an initiative by the Ministry of Health (MOH), is a 
  #' platform for all HCIs licensed under the Private Hospitals and Medical 
  #' Clinics (PHMC) Act to provide information about their services and 
  #' operations to the public.
  #' 
  #' This function is custom-made to consolidate the names and addresses of 
  #' HCIs which are medical clinics that offer general medical services.
  #' 
  #' The HCI Directory is a dynamic web page, so using RSelenium might be 
  #' required.
  #' 
  #' @return The names and addresses of selected HCIs.
  
  # Run a Selenium Server using `RSelenium::rsDriver()`. The parameters e.g. 
  #   `browser`, `chromever` (or `geckover` if using Firefox, or other drivers 
  #   if using other browsers) have to be properly set. Trial-and-error until a 
  #   configuration works. Set `check = T` the very first time it's run on a 
  #   system, then set `check = F` after that to speed things up.
  rD = RSelenium::rsDriver(browser = "chrome",
                           chromever = "83.0.4103.39",
                           check = F)
  
  # Connect to server with a remoteDriver instance.
  remDr = rD$client
  
  # Set timeout on waiting for elements
  remDr$setTimeout(type = "implicit", milliseconds = 10000)
  
  # Navigate to the given URL
  remDr$navigate("http://hcidirectory.sg/hcidirectory/")
  
  # Click 4 things:
  # 1. "MORE SEARCH OPTIONS"
  # 2. "Medical Clinics Only"
  # 3. "General Medical"
  # 4. "Search"
  c(
    "options" = "#moreSearchOptions",
    "medclins" = "#criteria > table > tbody > tr:nth-child(2) > td > label",
    "genmed" = "#isGenMed",
    "search" = "#search_btn_left"
  ) %>% 
    lapply(remDr$findElement, using = "css") %>% 
    purrr::walk(function(elem) elem$clickElement())
  
  # Find the number of pages
  results = remDr$findElement("#results", using = "css")
  npages = results$getElementAttribute("innerHTML")[[1]] %>% 
    xml2::read_html() %>% 
    rvest::html_node("#totalPage") %>% 
    rvest::html_attr("value") %>% 
    as.numeric()
  
  # Create an empty tibble to append results
  df = tibble::tibble(
    id = character(),
    name = character(),
    add = character()
  )
  
  i = 1
  while (T) {
    results = remDr$findElement("#results", using = "css")
    html = results$getElementAttribute("innerHTML")[[1]] %>% 
      xml2::read_html()
    
    # Determine the index numbers of the (up to 10) results on the page
    idx = html %>% 
      # Find the element that says "SHOWING 1 - 10 OF 1,761 RESULTS"
      rvest::html_nodes(".col1") %>% 
      .[1] %>% 
      rvest::html_text() %>% 
      # Commas have to be eliminated for numbers > 999
      gsub(",", "", .) %>% 
      # Find the smallest and largest numbers and apply the colon operator
      sub(".*Showing\\s+(.*)\\s+of.*", "\\1", .) %>% 
      strsplit(split = " - ") %>% 
      unlist() %>% 
      as.numeric() %>% 
      { .[1]:.[2] }
    
    # Only append results if IDs are not in the table
    if (!any(idx %in% df$id)) {
      df = df %>% 
        dplyr::bind_rows(
          html %>%
            # Find both the name and address nodes
            rvest::html_nodes(".name,.add") %>% 
            rvest::html_text() %>% 
            # Tidy whitespace
            gsub("\\s+", " ", .) %>% 
            trimws() %>% 
            # Concatenate IDs, odd rows (names), and even rows (addresses)
            { cbind(idx, .[c(TRUE,FALSE)], .[!c(TRUE,FALSE)]) } %>% 
            tibble::as_tibble() %>% 
            setNames(c("id", "name", "add"))
        )
      
      # Announce progress and increment page counter
      message(i, " of ", npages, " done (", round(i / npages * 100, 2), "%)")
      i = i + 1
    }
    
    # Natural exit point
    if (i > npages) break
    
    # Navigate to the next page (if available, else stop)
    the_end = tryCatch({
      nextpage = remDr$findElement("#PageControl > div.r_arrow", using = "css")
      nextpage$clickElement()
      F
    }, error = function(e) {
      print(paste("There are no more pages after", i))
      T
    })
    
    # Unnatural exit point
    if (the_end) break
  }
  
  # Clean up RSelenium
  remDr$close()
  rD[["server"]]$stop()
  rm(rD, remDr)
  gc()
  
  # Kill Java instance(s) inside RStudio
  # docs.microsoft.com/en-us/windows-server/administration/windows-commands/taskkill
  system("taskkill /im java.exe /f", intern = F, ignore.stdout = F)
  
  # Clean up:
  # - Franchises may have the same name with different addresses
  # - Different practices may have the same zipcodes and even buildings
  # - We will consider each full address unique, and a single practice
  
  # Clean up duplicate addresses
  df %>% 
    .[!duplicated(tolower(.$add), fromLast = T),]
}

zipcodes_to_geocodes <- function(zipcodes) {
  #' Get Geo-location from Google Maps
  #' 
  #' @description
  #' Attempt to obtain the longitudes, latitudes, and addresses of the given 
  #' zipcodes using ggmap::geocode().
  #' 
  #' @param zipcodes A vector of zipcodes.
  #' @return Geo-location data of the associated zipcodes.
  
  # Prompt user to input API key
  ggmap::register_google(key = readline("Please enter Google API key: "))
  
  # Create an (almost) empty tibble to append results
  res = zipcodes %>% 
    # Remove duplicates to minimize number of requests
    .[!duplicated(.)] %>% 
    tibble::as_tibble() %>% 
    dplyr::rename(zip = value) %>% 
    dplyr::mutate(lon = NA_real_,
                  lat = NA_real_,
                  address = NA_character_)
  
  for (i in 1:nrow(res)) {
    result = tryCatch({
      ggmap::geocode(res$zip[i], output = "latlona", source = "google")
    }, warning = function(w) {
      w$message
    }, error = function(e) {
      NA
    })
    
    # If the registered key is invalid, there's no point continuing
    if (grepl("The provided API key is invalid", result[1], fixed = T)) {
      stop("A valid Google API key is required.")
    }
    
    # A useful result will have something, and will have names
    if (!is.na(result) && !is.null(names(result))) {
      res$lon[i] = result$lon
      res$lat[i] = result$lat
      res$address[i] = result$address
    }
    
    # Announce progress
    message(i, " of ", nrow(res), " (",round(i / nrow(res) * 100, 2), "%)")
  }
  
  res
}
```

The following code chunk has been disabled, but is provided here to show how the data may be obtained from primary sources (with one exception). Using this code chunk would require activating the previous code chunk, as well as a valid Google Maps Platform API key.

```{r grand-import-de-novo, eval=FALSE}
grand_import_de_novo <- function() {
  # This might take a while, and requires a Google Maps Platform API key
  
  list(
    "moh_bulletin" = import_moh_weekly(),
    
    "mss_19stations" = import_mss_daily(years = 2012:2020),
    
    "hci_clinics" = import_hcidirectory() %>% 
      dplyr::mutate(zip = sub(".*((?i)singapore\\s+\\d+).*", "\\1", add)) %>% 
      # Requires Google Maps Platform API key
      dplyr::left_join(zipcodes_to_geocodes(.$zip), by = "zip") %>% 
      dplyr::select(-id, -zip, -address) %>% 
      tidyr::drop_na(),
    
    "planning_areas" = read_kmls(paste0(
      "https://geo.data.gov.sg/plan-bdy-dwelling-type-2017/2017/09/27/kml/",
      "plan-bdy-dwelling-type-2017.kml"
    )),
    
    "regions" = read_kmls(paste0(
      "https://geo.data.gov.sg/mp14-region-web-pl/2014/12/05/kml/",
      "mp14-region-web-pl.zip"
    )),
    
    "dengue_polys" = read_kmls(
      c("central", "northeast", "southeast", "southwest") %>% 
        paste0("https://geo.data.gov.sg/denguecase-", .,
               "-area/2020/07/17/kml/denguecase-", ., "-area.kml")
    ),
    
    "aedes_polys" = read_kmls(c(
      c("central", "northeast", "northwest") %>% 
        paste0("https://geo.data.gov.sg/breedinghabitat-", .,
               "-area/2020/07/17/kml/breedinghabitat-", ., "-area.kml"),
      c("southeast", "southwest") %>% 
        paste0("https://geo.data.gov.sg/breedinghabitat-", .,
               "-area/2020/07/23/kml/breedinghabitat-", ., "-area.kml")
    )),
    
    "mss_63station_pos" = readr::read_csv(paste0(
      "https://raw.githubusercontent.com/roscoelai/dasr2020capstone/master/",
        "data/mss/Station_Records.csv"
    ))
  )
}

# raw_data <- grand_import_de_novo()
```

### Import data from saved files

If the data files are available locally, set `from_online_repo = F` for faster loading. Otherwise, the file will be obtained from an online repository. The .csv files will be read directly while the other file formats will be downloaded to a temporary folder and read locally.

```{r grand-import-no-webscraping, results='hide', message=FALSE}
grand_import_no_webscraping <- function(from_online_repo = TRUE) {
  # Allow user to choose whether to import raw data from an online repository 
  #   or from local files.
  
  if (from_online_repo) {
    fld = paste0("https://raw.githubusercontent.com/roscoelai/",
                 "dasr2020capstone/master/data/")
  } else {
    # Check that the "../data/ folder exists
    assertthat::assert_that(dir.exists("../data/"),
                            msg = 'Unable to locate "../data/" directory.')
    fld = "../data/"
  }
  
  list(
    "moh_bulletin" = import_moh_weekly(paste0(
      fld, "moh/weekly-infectious-disease-bulletin-year-2020.xlsx"
    )),
    
    "mss_19stations" = readr::read_csv(paste0(
      fld, "mss/mss_daily_2012_2020_19stations_20200728.csv"
    )),
    
    "hci_clinics" = readr::read_csv(paste0(
      fld, "hcid/hci_clinics_20200725.csv"
    )),
    
    "planning_areas" = read_kmls(paste0(
      fld, "kmls/plan-bdy-dwelling-type-2017.kml"
    )),
    
    "regions" = read_kmls(paste0(
      fld, "kmls/MP14_REGION_WEB_PL.kml"
    )),
    
    "dengue_polys" = read_kmls(paste0(
      fld, "kmls/denguecase-", c("central",
                                 "northeast",
                                 "southeast",
                                 "southwest"), "-area.kml"
    )),
    
    "aedes_polys" = read_kmls(paste0(
      fld, "kmls/breedinghabitat-", c("central",
                                      "northeast",
                                      "northwest",
                                      "southeast",
                                      "southwest"), "-area.kml"
    )),
    
    "mss_63station_pos" = readr::read_csv(paste0(
      fld, "mss/Station_Records.csv"
    ))
  )
}

raw_data <- grand_import_no_webscraping(from_online_repo = F)
```

### What raw data do we have?

```{r}
raw_data %>% 
  tibble::tibble(
    names = names(.),
    data = .,
    nrow = sapply(., nrow),
    ncol = sapply(., ncol)
  )

raw_data
```

---

## Tidy & Transform

```{r}
transpose_html_table <- function(html) {
  xml2::read_html(html) %>% 
    rvest::html_node("table") %>% 
    rvest::html_table() %>% 
    t() %>% 
    `colnames<-`(.[1,]) %>% 
    .[2,]
}

idw_interpolation <- function(points, points_label,
                              polys, polys_label,
                              ordinal = 2) {
  #' Inverse-distance-weighted (IDW) Interpolation
  #' 
  #' @description
  #' Calculate the inverse-distance-weighted (IDW) averages of the values of a 
  #' given set of points to the centroids of a given set of polygons.
  #' 
  #' The ordinal determines the amount of weight given to inverse-distances. 
  #' An ordinal of 0 will return the same value (the arithmetic mean) to all 
  #' centroids. A large ordinal will heavily penalize points that are far away. 
  #' A negative ordinal will result in distance-weighted interpolation - please 
  #' don't do that.
  #' 
  #' @param points sf object containing point features.
  #' @param polys sf object containing polygon features.
  #' @param points_label Column name of the labels in points.
  #' @param polys_label Column name of the labels in polys.
  #' @param ordinal Determines the amount of weight given to inverse-distances.
  #' @return Interpolated values for each polygon.
  
  if (ordinal < 0) {
    stop("You are highly encouraged to use a non-negative ordinal.")
  }
  
  # weights: (nrow(polys) x nrow(points))
  weights = polys %>% 
    sf::st_centroid() %>% 
    sf::st_distance(points) %>% 
    units::set_units(km) %>% 
    { 1 / (.^ordinal) }
  
  # values: (nrow(points) x ncol(values))
  values = points %>% 
    as.data.frame() %>% 
    dplyr::select(-{{ points_label }}, -geometry) %>% 
    as.matrix()
  
  # result: (nrow(polys) x ncol(values))
  result = tibble::as_tibble(weights %*% values / rowSums(weights))
  
  polys %>% 
    tibble::as_tibble() %>% 
    dplyr::select({{ polys_label }}) %>% 
    dplyr::bind_cols(result)
}

find_superset <- function(sub, super, super_label, sub_label = ".") {
  sub %>% 
    sf::st_centroid() %>% 
    sf::st_intersects(super) %>% 
    tibble::as_tibble() %>% 
    dplyr::transmute(
      {{ sub_label }} := sub[[sub_label]][row.id],
      {{ super_label }} := super[[super_label]][col.id]
    )
}

# Store intermediate processed data in this list
data <- list()
```

We work towards 2 datasets:

1. `data_time` containing epidemiological and meteorological data from 2012 to 2020
2. `data_space` containing number of dengue cases, _Aedes_ mosquito habitats, clinics, population dwelling types, and most recent meteorological data broken down by the planning areas in Singapore

### `data_time`

The unit of analysis for `data_time` is the epidemiological week.

The epidemiological data (MOH bulletin) is already organized this way.

The meteorological data is organized by days for each of 19 climate stations. To match the epidemiological data, it has to be aggregated to obtain representative values at the national level by epidemiological weeks.

In this way, how many values are we aggregating for each variable, for each week?

```{r}
raw_data$mss_19stations %>% 
  tidyr::pivot_longer(Rainfall:Temp_range) %>% 
  tidyr::drop_na() %>% 
  dplyr::count(Epiyear, Epiweek, name) %>% 
  dplyr::select(name, n) %T>% 
  { print(table(.)) } %>% 
  ggplot(aes(x = n, color = name)) +
  geom_histogram(binwidth = 1) +
  facet_grid(name ~ ., scales = "free") + 
  labs(x = "Number of values", y = "Number of weeks") + 
  theme(legend.position = "none")
```

Most weeks will be aggregating more than 100 values for all 3 variables.

Now we join the datasets to get `data_time`.

```{r}
data[["moh_bulletin_s"]] <- raw_data$moh_bulletin %>% 
  dplyr::select(
    Epiyear:End,
    Dengue,
    HFMD,
    `Acute Upper Respiratory Tract infections`,
    `Acute Diarrhoea`
  ) %>% 
  dplyr::rename(
    URTI = `Acute Upper Respiratory Tract infections`,
    Diarrhoea = `Acute Diarrhoea`
  )
```

Take a break to clean data for MOH bulletin

```{r}
# Find the row(s) with error(s)
mask <- data$moh_bulletin_s$End %>% 
  { . < "2012-01-01" | . > "2021-01-03" }

# Personally inspect the erroneous row(s)
data$moh_bulletin_s[mask,]

# Correct error(s)
data$moh_bulletin_s[mask, "End"] <- 
  data$moh_bulletin_s$Start[mask] + lubridate::days(6)

# Check correction(s)
data$moh_bulletin_s[mask,]
```

```{r}
data[["mss_19stations_s"]] <- raw_data$mss_19stations %>% 
  dplyr::group_by(Epiyear, Epiweek) %>% 
  dplyr::summarise(
    Mean_rainfall = mean(Rainfall, na.rm = T),
    Med_rainfall = median(Rainfall, na.rm = T),
    Mean_temp = mean(Mean_temp, na.rm = T),
    Med_temp = median(Mean_temp, na.rm = T),
    Mean_temp_rng = mean(Temp_range, na.rm = T),
    Med_temp_rng = median(Temp_range, na.rm = T),
    .groups = "drop"
  )

data_time <- dplyr::left_join(
  data$moh_bulletin_s,
  data$mss_19stations_s,
  by = c("Epiyear", "Epiweek")
) %>% 
  tidyr::drop_na()

data_time

dplyr::glimpse(data_time)
```

`data_time` contains 10 features: 4 indicators of disease numbers and 6 aggregated meteorological measures. Each value is associated with an epidemiological week. The time period spans from 2012-W01 to 2020-W30. As the meteorological records for July 2020 have not been published, there are no corresponding values from 2020-W28 onward.

### `data_space`

> **A lot** more wrangling is involved.

The unit of analysis for `data_space` is the planning area. This is specified by `raw_data$planning_areas`, and the other datasets would have to be transformed to match before merging.

Spatial analysis would be rather limited and more emphasis placed on visualization.

```{r, results='hide', message=FALSE}
# Polygons are 200 m X 200 m squares. The `Description` contains the number 
#   of occurrences within each polygon. Use a suitable regular expression 
#   pattern to extract the number for each polygon. Find the centroid for 
#   each polygon, then duplicate each row by the number of occurrences.
data[c("dengue_points", "aedes_points")] <- 
  raw_data[c("dengue_polys", "aedes_polys")] %>% 
  lapply(function(sf_tbl_df) {
    sf_tbl_df %>% 
      dplyr::transmute(n = sub(".*: (\\d+).*", "\\1", Description)) %>% 
      sf::st_centroid() %>% 
      .[rep(1:nrow(.), as.numeric(.$n)),]
  })

data[["clinic_points"]] <- raw_data$hci_clinics %>% 
  sf::st_as_sf(coords = c("lon", "lat")) %>% 
  sf::`st_crs<-`("WGS84")

data[["weather_points"]] <- raw_data$mss_19stations %>% 
  dplyr::filter(Epiyear == 2020) %>% 
  dplyr::filter(Epiweek > max(Epiweek) - 4) %>% 
  dplyr::group_by(Station) %>% 
  dplyr::summarise(mean_rainfall = mean(Rainfall, na.rm = T),
                   med_rainfall = median(Rainfall, na.rm = T),
                   mean_temp = mean(Mean_temp, na.rm = T),
                   med_temp = median(Mean_temp, na.rm = T),
                   mean_temp_rng = mean(Temp_range, na.rm = T),
                   med_temp_rng = median(Temp_range, na.rm = T),
                   .groups = "drop") %>% 
  tidyr::drop_na() %>% 
  dplyr::left_join(
    dplyr::select(raw_data$mss_63station_pos, Station:`Long. (E)`),
    by = "Station"
  ) %>% 
  sf::st_as_sf(coords = c("Long. (E)", "Lat.(N)")) %>% 
  sf::`st_crs<-`("WGS84")

data[["planning_areas"]] <- raw_data$planning_areas %>% 
  dplyr::bind_cols(
    .$Description %>% 
      lapply(transpose_html_table) %>% 
      dplyr::bind_rows()
  ) %>% 
  tibble::as_tibble() %>% 
  dplyr::rename_all(tolower) %>% 
  dplyr::rename(plan_area = pln_area_n,
                pop = total) %>% 
  dplyr::mutate(plan_area = tools::toTitleCase(tolower(plan_area)),
                dplyr::across(pop:others, as.numeric)) %>% 
  dplyr::select(-matches("^(name|description|inc|fmel)")) %>% 
  sf::st_as_sf()

data[["regions"]] <- raw_data$regions %>% 
  dplyr::mutate(Name = tools::toTitleCase(tolower(Name)),
                Name = sub(" Region", "", Name)) %>% 
  dplyr::rename(region = Name) %>% 
  dplyr::select(-Description)

# This is long
data_space <- data$planning_areas %>% 
  dplyr::select(-one_to_two_rm:-five_rm_exec_flats, -others) %>% 
  
  # Regions
  dplyr::left_join(
    # Changi Bay and Western Islands not assigned, but inconsequential, for now
    find_superset(
      sub = .,
      super = data$regions,
      super_label = "region",
      sub_label = "plan_area"
    ),
    by = "plan_area"
  ) %>% 
  
  # Counts
  dplyr::left_join(
    list(
      ncases = data$dengue_points,
      nhabs = data$aedes_points,
      nclinics = data$clinic_points
    ) %>% {
      lapply(names(.), function(name) {
        find_superset(
          sub = .[[name]],
          super = data$planning_areas,
          super_label = "plan_area"
        ) %>% 
          dplyr::count(plan_area, name = name)
      })
    } %>% 
      Reduce(function(x, y) dplyr::left_join(x, y, by = "plan_area"), .)
  ) %>% 
  
  dplyr::select(plan_area, region, ncases, nhabs, nclinics, everything()) %>% 
  
  # Meteorological data
  dplyr::left_join(
    idw_interpolation(
      points = data$weather_points,
      points_label = "Station",
      polys = .,
      polys_label = "plan_area",
      ordinal = 10
    ),
    by = "plan_area"
  ) %>% 
  
  # Calculations
  dplyr::mutate(
    area_km2 = units::set_units(sf::st_area(.), km^2),
    ncases_p100k = ncases / pop * 100000,
    nclinics_p100k = nclinics / pop * 100000,
    hdb_pp = hdb / pop,
    condos_other_apts_pp = condos_other_apts / pop,
    landed_properties_pp = landed_properties / pop,
    pop_pkm2 = as.numeric(pop / area_km2),
    ncases_pkm2 = as.numeric(ncases / area_km2),
    nclinics_pkm2 = as.numeric(nclinics / area_km2),
    nhabs_pkm2 = as.numeric(nhabs / area_km2),
    label = paste0(
      "<b>", plan_area, "</b><br/>",
      "Area (km<sup>2</sup>): ", round(area_km2, 2), "<br/>",
      "Population: ", pop, "<br/>",
      "Cases: ", ncases, "<br/>",
      "Clinics: ", nclinics, "<br/>",
      "<i>Aedes</i> habitats: ", nhabs, "<br/>",
      "Cases (per 100k): ", round(ncases_p100k, 2), "<br/>",
      "Cases (per km<sup>2</sup>): ", round(ncases_pkm2, 2)
    )
  )

data_space

dplyr::glimpse(data_space)
```

`data_space` contains 14 main and 9 derived features for each planning area (URA MP14), including: number of dengue cases, number of _Aedes_ mosquito habitats, number of clinics, population, various dwelling types, area, and  meteorological variables. Dengue case data were unavailable for several planning areas, especially since data for the north-west region were not published.

---

## Explore

```{r}
plot_heatmap <- function(.data, y, low_color, high_color, subtitle = "") {
  .data %>% 
    dplyr::mutate(
      month = factor(
        lubridate::month(End),
        levels = as.character(1:12),
        labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                   "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
        ordered = TRUE
      )
    ) %>% 
    dplyr::select(-matches("week|Start|End|Mean|Med")) %>% 
    tidyr::pivot_longer(c(-Epiyear, -month)) %>% 
    dplyr::filter(name == y) %>% 
    dplyr::group_by(Epiyear, month) %>% 
    dplyr::summarise(Cases = sum(value),
                     .groups = "drop") %>% 
    ggplot(aes(x = month, y = Cases, fill = Cases)) + 
    geom_col() + 
    scale_fill_gradient(low = low_color, high = high_color) +
    facet_grid(Epiyear ~ .) + 
    labs(
      title = paste0("Number of ", y, " Cases from 2012-2020"),
      subtitle = subtitle,
      x = "Month",
      y = "Cases",
      caption = "Source: Ministry of Health"
    ) + 
    theme(legend.position = "none")
}

plot_choropleths <- function(.data, .vars) {
  gridExtra::grid.arrange(grobs = lapply(.vars, function(var) {
    ggplot(aes(fill = .data[[var]]), data = .data) + 
      geom_sf() + 
      scale_fill_viridis_c(guide = guide_colourbar(
        title.position = "top",
        title.hjust = .5,
        barwidth = 10,
        barheight = .5
      )) +
      theme_void() + 
      theme(legend.position = "bottom")
  }), ncol = length(.vars))
}

plot_ggbetweenstats <- function(data, x, y, xlab, ylab) {
  type = ifelse(shapiro.test(data_space[[y]])$p.value < 0.05, "np", "p")
  
  ggstatsplot::ggbetweenstats(
    data = data,
    x = {{ x }},
    y = {{ y }},
    xlab = {{ xlab }},
    ylab = {{ ylab }},
    type = type,
    plot.type = "box",
    effsize.type = "biased",
    nboot = 50,
    pairwise.comparisons = T, 
    pairwise.display = "significant",
    pairwise.annotation = "p.value",
    p.adjust.method = "bonferroni"
  )
}
```

### Time

```{r}
plot_heatmap(
  data_time,
  y = "Dengue",
  low_color = "ivory3",
  high_color = "deeppink4",
  subtitle = "Highest number of Cases in 2020"
)

plot_heatmap(
  data_time,
  y = "HFMD",
  low_color = "lightskyblue",
  high_color = "darkblue"
)

plot_heatmap(
  data_time,
  y = "URTI",
  low_color = "seagreen2",
  high_color = "honeydew4"
)

plot_heatmap(
  data_time,
  y = "Diarrhoea",
  low_color = "lightyellow",
  high_color = "tomato4"
)
```

```{r}
# data_time %T>% 
#   dplyr::glimpse() %>% 
#   dplyr::select(-matches("Epiweek|End")) %>% 
#   dplyr::mutate(Epiyear = as.factor(Epiyear)) %>% 
#   tidyr::pivot_longer(Dengue:Med_temp_rng) %>% 
#   ggplot(aes(x = Start, y = value, color = Epiyear)) + 
#   geom_line() + 
#   geom_point(alpha = 0.3) + 
#   facet_grid(name ~ ., scales = "free_y") + 
#   labs(title = "Weekly history of variables from 2012 to 2020",
#        x = "",
#        y = "",
#        caption = "Sources: moh.gov.sg, weather.gov.sg") + 
#   theme(legend.position = "none")
```

In general, the number of cases of URTI and diarrhea are consistent throughout the years; HFMD saw a sharp decrease from 2019 and DF fluctuated over the years, we observed a dip in number of Dengue cases in 2017 and a sudden spike in 2020. The decrease in dengue cases in 2017 could be attributed to several reasons such as, increase effort by NEA and the community in response to Zika outbreak in 2016 and perhaps the local population has built up immunity after outbreaks of dengue fever in the past. The spike in 2020 could be explained by the prolong period of time staying at home and the lack of resistance to a newly circulating strain of dengue virus.

![](https://raw.githubusercontent.com/roscoelai/dasr2020capstone/master/imgs/choropleth_mean_temp_ord_15.gif)

### Space

```{r, fig.height=3}
plot_choropleths(data_space, c("ncases", "ncases_p100k", "ncases_pkm2"))
```

```{r, fig.height=3}
plot_choropleths(data_space, c("nhabs", "nhabs_pkm2"))
```

```{r, fig.height=3}
plot_choropleths(data_space, c("nclinics", "nclinics_pkm2"))
```

```{r, fig.height=3}
plot_choropleths(data_space, c("mean_rainfall", "med_temp", "med_temp_rng"))
```

```{r, fig.height=7}
plot_ggbetweenstats(
  data = data_space,
  x = "region",
  y = "ncases",
  xlab = "Regions in Singapore",
  ylab = "Number of cases"
)
```

```{r, fig.height=7}
plot_ggbetweenstats(
  data = data_space,
  x = "region",
  y = "ncases_p100k",
  xlab = "Regions in Singapore",
  ylab = "Number of cases (per 100,000)"
)
```

```{r, fig.height=7}
plot_ggbetweenstats(
  data = data_space,
  x = "region",
  y = "med_temp",
  xlab = "Regions in Singapore",
  ylab = "Median temperature"
)
```

```{r, fig.height=7}
plot_ggbetweenstats(
  data = data_space,
  x = "region",
  y = "med_temp_rng",
  xlab = "Regions in Singapore",
  ylab = "Median temperature range"
)
```

```{r}
data_space %>% 
  tibble::as_tibble() %>% 
  dplyr::select(ncases, nhabs, hdb:landed_properties) %>% 
  tidyr::drop_na() %>% 
  as.matrix() %>% 
  Hmisc::rcorr() %>% 
  broom::tidy() %>% 
  dplyr::filter(p.value < 0.05) %>% 
  dplyr::arrange(estimate)
```

```{r}
data_space %>% 
  tibble::as_tibble() %>% 
  dplyr::select(ncases, nhabs, mean_rainfall, med_temp, med_temp_rng) %>% 
  tidyr::drop_na() %>% 
  as.matrix() %>% 
  Hmisc::rcorr() %>% 
  broom::tidy() %>% 
  dplyr::filter(p.value < 0.05) %>% 
  dplyr::arrange(estimate)
```

Leaflet map of case density (per km^2^) choropleth with individual case clusters overlay.

```{r}
data_space %>% 
  leaflet::leaflet(width = "100%") %>%
  leaflet::addTiles() %>%
  leaflet::addPolygons(
    weight = 1,
    opacity = 1,
    fillOpacity = 0.7,
    smoothFactor = 0.5,
    fillColor = ~leaflet::colorNumeric("Reds", ncases_pkm2)(ncases_pkm2),
    label = ~lapply(label, htmltools::HTML),
    popup = ~lapply(label, htmltools::HTML)
  ) %>%
  leaflet::addCircleMarkers(
    data = data$dengue_points,
    color = "red",
    radius = 5,
    fillOpacity = 0.5,
    clusterOptions = leaflet::markerClusterOptions()
  ) %>% 
  leaflet::addLabelOnlyMarkers(
    data = sf::st_centroid(data_space),
    label =  ~plan_area,
    labelOptions = leaflet::labelOptions(
      noHide = T,
      textOnly = T,
      direction = "center",
      style = list("color" = "blue"))
  )
```

---

## Model

### Dengue

Let's plot the x variables (Mean_rainfall, Med_temp, Med_temp_rng) and Y (Dengue)

```{r, fig.height=9}
{par(mfrow = c(3, 2))
  hist(data_time$Dengue, breaks=40)
  hist(data_time$Mean_rainfall, breaks=40)
  hist(data_time$Mean_temp, breaks=40)
  hist(data_time$Med_temp, breaks=40)
  hist(data_time$Mean_temp_rng, breaks=40)
  hist(data_time$Med_temp_rng, breaks=40)
}

ks.test(data_time$Mean_temp, data_time$Med_temp)
ks.test(data_time$Mean_temp_rng, data_time$Med_temp_rng)
```

`Mean_temp` and `Med_temp` have similar distributions, and `Mean_temp_rng` and `Med_temp_rng` have similar distributions, therefore `Med_temp` and `Med_temp_rng` were used.

Let's run a correlation model to see the relationship between variables

```{r}
data_time %>% 
  dplyr::select(Dengue, Mean_rainfall, Med_temp, Med_temp_rng) %>%
  as.matrix() %>%
  Hmisc::rcorr() %>%
  broom::tidy() %>% 
  # dplyr::filter(p.value < 0.05) %>% 
  dplyr::mutate(dplyr::across(is.numeric, ~round(., 3))) %>% 
  dplyr::arrange(estimate)
```

Run a linear regression model

```{r}
model1_Dengue <- lm(Dengue ~ Mean_rainfall + Med_temp + Med_temp_rng,
                    data = data_time)

broom::tidy(model1_Dengue) %>% 
  dplyr::mutate(dplyr::across(is.numeric, ~round(., 3)))
```

Let's check the skewness of y variable

```{r}
data_time %>% 
  tidyr::drop_na() %>% 
  { e1071::skewness(.$Dengue) }  # [1] 2.033787
```

Let's see how the density plots look like after transformation

```{r}
{
  par(mfrow = c(2, 2))
  plot(density(data_time$Dengue, na.rm = T), main = "untransformed")
  plot(density(sqrt(data_time$Dengue), na.rm = T), main = "sqrt")
  plot(density(log10(data_time$Dengue), na.rm = T), main = "log10")
  plot(density(1 / data_time$Dengue, na.rm = T), main = "inverse")
}
```

Not really good. Try them on Model 1

```{r}
model1_log_Dengue <- lm(log10(Dengue)~Mean_rainfall+Med_temp+Med_temp_rng,
                    data=data_time)
broom::tidy(model1_log_Dengue)
gvlma::gvlma(model1_log_Dengue)
```

```{r}
model1_sqrt_Dengue <- lm(sqrt(Dengue)~Mean_rainfall+Med_temp+Med_temp_rng,
                       data=data_time)
broom::tidy(model1_sqrt_Dengue)
gvlma::gvlma(model1_sqrt_Dengue)
```

```{r}
model1_inv_Dengue <- lm((1/Dengue)~Mean_rainfall+Med_temp+Med_temp_rng,
                        data=data_time)
broom::tidy(model1_inv_Dengue)
gvlma::gvlma(model1_inv_Dengue)
```

Let's have a look at the models' adjusted R2 values

```{r}
list(
  "untransformed" = model1_Dengue,
  "log" = model1_log_Dengue,
  "sqrt" = model1_sqrt_Dengue,
  "inv" = model1_inv_Dengue
) %>% 
  lapply(broom::glance) %>% 
  { dplyr::bind_cols(names(.), dplyr::bind_rows(.)) } %>% 
  dplyr::rename(name = ...1)
```

It seems that `model1_Dengue` (untransformed) has a higher adjusted R2 value than the other models, and it explains about 6% of the variance.

Plot the regression models

```{r}
data_time %>% 
  dplyr::select(Dengue, Med_temp, Med_temp_rng, Mean_rainfall) %>% 
  # scale() %>% 
  tibble::as_tibble() %>% 
  tidyr::pivot_longer(-Dengue) %>% 
  ggplot(aes(x = value, y = Dengue, color = name)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", formula = y ~ x, color = "blue") + 
  facet_grid(. ~ name, scales = "free") + 
  theme(legend.position = "none")
```

![24 hr temperature mean per month is 27.5 mean Rainfall is 13.8](http://www.weather.gov.sg/climate-climate-of-singapore/#:~:text=Rainfall%20is%20plentiful%20in%20Singapore,is%200.2mm%20or%20more.%5D)

Since rainfall has marginal significance at an alpha of 10% and known to have effect on vector born diseases, we decided to explore rainfall variable by categorizing it (<11mm of rain fall and >=11mm of rainfall)

```{r}
data_time$Mean_rainfall %>% 
  hist()
```

```{r}
data_time <- data_time %>% 
  dplyr::mutate(Rain_11 = ifelse(Mean_rainfall >= 11, 1, 0))

# glimpse(data_time)
```

Re-Run Model1 With categorised Rain variable

```{r}
model2_Dengue <- lm(Dengue~Rain_11+Med_temp+Med_temp_rng,
                                      data=data_time)

gvlma::gvlma(model2_Dengue)
broom::tidy(model2_Dengue)
```

Add Interaction term

```{r}
model2_Dengue_Int <- lm(Dengue ~ (Med_temp + Med_temp_rng) * Rain_11,
                       data=data_time)

gvlma::gvlma(model2_Dengue_Int)
broom::tidy(model2_Dengue_Int)
anova(model2_Dengue,model2_Dengue_Int)
broom::glance(model2_Dengue)
broom::glance(model2_Dengue_Int)
```

There is an interaction effect (Rain_11 on 2 other predictors) and Anova test shows adding the interaction terms improves the model fit (p value=0.0017).
This interaction model has a higher R2 value too.

r.squared(Model2_Dengue) --> 0.0757
r.squared(Model2_Dengue_Int) --> 0.102
adj.r.squared(Model2_Dengue) --> 0.0694
adj.r.squared(Model2_Dengue_Int) --> 0.0919

Unfortunately, all model assumptions failed. So This model won't be a good one for prediction

Lets explore this interaction model

```{r}
#Report the model summary and significance using export_summs() function
library(jtools)
jtools::export_summs(model2_Dengue,model2_Dengue_Int,
             error_format = "(p = {p.value})",
             model.names = c("Main Effects Only",
                             "Interaction Effects"),
             digits = 3)
```

Probing Interaction Effects
Run Spotlight analysis, using Johnson-Neyman Techniques

```{r, fig.height=7, fig.width=9}
#  a. Median Temperature
interactions::sim_slopes(model2_Dengue_Int, 
           pred = Rain_11,
           modx = Med_temp_rng, 
           johnson_neyman = T) #7.01

interactions::interact_plot(model2_Dengue_Int,
              pred="Med_temp",
              modx = "Rain_11",
              modx.labels = c("less than 11mm of rain fall", 
                              "rain fall >=11mm"),
              interval = T,
              int.width = .95,
              colors = c("deeppink",
                         "darkgreen"),
              vary.lty = T,
              line.thickness = 1,
              legend.main = "") +
  ggplot2:: geom_vline(xintercept = 27.15, col = "red", linetype = 3, size = 1)+
  labs(title = "Effect of Rain fall and Temperature on Dengue cases in Singapore",
       subtitle = "With an average weekly rainfall of 11mm and above,the number of Dengue cases\nincreases with rise in temperature", 
       x = "Weekly Median Temperature",
       y = "Number of Dengue Cases",
       caption = "Source: MOH, NEA Websites") +
  annotate("text", 
           x = 28, 
           y = 0,
           label = "The shaded areas denote 95% confidence intervals.\nThe vertical line marks the boundary\nbetween regions of significance and non-significance\nbased on alpha at 5%") + 
  theme(legend.position = "top")
```

```{r, fig.height=7, fig.width=9}
#  b. Median Temperature Range
interactions::sim_slopes(model2_Dengue_Int, 
           pred = Rain_11,
           modx = Med_temp, 
           johnson_neyman = T) #27.15

interactions::interact_plot(model2_Dengue_Int,
              pred="Med_temp_rng",
              modx = "Rain_11",
              modx.labels = c("less than 11mm of rain fall", 
                              "rain fall >=11mm"), 
              interval = T,
              int.width = .95,
              colors = c("deeppink",
                         "darkgreen"),
              vary.lty = T,
              line.thickness = 1,
              legend.main = "Rain Fall")+
  ggplot2:: geom_vline(xintercept = 7.01, col = "red", linetype = 3, size = 1)+
  labs(title = "Effect of Rain fall and Temperature Variation on Dengue Cases in Singapore",
       subtitle = "With an average weekly rainfall of 11mm and above,the number of Dengue cases\ndecreases when tempeature variation is HIGH", 
       x = "Weekly Median Temperature Range",
       y = "Number of Dengue Cases",
       caption = "Source: MOH, NEA Websites") +
  annotate("text", 
           x = 6, 
           y = 100,
           label = "The shaded areas denote 95% confidence intervals.\nThe vertical line marks the boundary\nbetween regions of significance and non-significance\nbased on alpha at 5%") + 
  theme(legend.position = "top")
```

### Upper Respiratory Tract Infections (URTI)

Since the number of URTI cases in Singapore drastically reduced due to COVID-19, we decided to exclude the values since April 2020 (from circuit breaker period), for this analysis

Let's plot the x variables (Mean_rainfall,Med_temp,Med_temp_rng) and Y (URTI)

```{r, fig.height=9}
data_time_URTI <- data_time %>% 
  dplyr::filter(URTI >= 2000)

{
  par(mfrow = c(2, 2))
  hist(data_time_URTI$Mean_rainfall, breaks=40)
  hist(data_time_URTI$Med_temp, breaks=40)
  hist(data_time_URTI$Med_temp_rng, breaks=40)
  hist(data_time_URTI$URTI, breaks=40)
}
```

Let's run a correlation model to see the relationship between variables

```{r}
data_time_URTI %>% 
  dplyr::select(URTI, Mean_rainfall, Med_temp, Med_temp_rng) %>%
  as.matrix() %>%
  Hmisc::rcorr() %>%
  broom::tidy() %>% 
  # dplyr::filter(p.value < 0.05) %>% 
  dplyr::mutate(dplyr::across(is.numeric, ~round(., 3))) %>% 
  dplyr::arrange(estimate)
```

Run a simple linear regression model

```{r}
model1_URTI <- lm(URTI~Mean_rainfall+Med_temp+Med_temp_rng,
                    data=data_time_URTI)

broom::tidy(model1_URTI)
gvlma::gvlma(model1_URTI)
```

Assumption checks failed
Let's check the skewness of y variable

```{r}
e1071::skewness(data_time_URTI$URTI) #[1] 0.8635183
```

```{r}
#Lets see how the density plots look like after transformation
{
  par(mfrow = c(2, 2))
  plot(density(data_time_URTI$URTI), main = "untransformed")
  plot(density(sqrt(data_time_URTI$URTI)), main = "sqrt")
  plot(density(log10(data_time_URTI$URTI)), main = "log10")
  plot(density(1 / data_time_URTI$URTI), main = "inverse")
}
```

Try them on Model 1

```{r}
model1_log_URTI <- lm(log10(URTI)~Mean_rainfall+Med_temp+Med_temp_rng,
                        data=data_time_URTI)
broom::tidy(model1_log_URTI)
gvlma::gvlma(model1_log_URTI)
```

```{r}
model1_sqrt_URTI <- lm(sqrt(URTI)~Mean_rainfall+Med_temp+Med_temp_rng,
                         data=data_time_URTI)
broom::tidy(model1_sqrt_URTI)
gvlma::gvlma(model1_sqrt_URTI)
```

```{r}
model1_inv_URTI <- lm((1/URTI)~Mean_rainfall+Med_temp+Med_temp_rng,
                        data=data_time_URTI)
broom::tidy(model1_inv_URTI)
gvlma::gvlma(model1_inv_URTI)
```

Assumptions acceptable

Plot the regression model 1

```{r}
data_time_URTI %>% 
  dplyr::select(URTI, Med_temp, Med_temp_rng,Mean_rainfall) %>% 
  # scale() %>% 
  tibble::as_tibble() %>% 
  tidyr::pivot_longer(-URTI) %>% 
  ggplot(aes(x = value, y = URTI, color = name)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", formula = y ~ x, color = "blue") + 
  facet_grid(. ~ name, scales = "free") + 
  theme(legend.position = "none")
```

From the plot, there are more cases when rain fall is lesser. So we decided to explore rainfall variable by categorizing it. (<11cm of rain fall and >=11cm of rainfall))

```{r}
data_time_URTI <- data_time_URTI %>% 
  dplyr::mutate(Rain_11 = ifelse(Mean_rainfall >= 11, 1, 0))

# glimpse(data_time_URTI)
```

Re-Run Model1 With categorised Rain variable

```{r}
model2_inv_URTI<- lm((1/URTI)~Rain_11+Med_temp+Med_temp_rng,
                   data=data_time_URTI)

gvlma::gvlma(model2_inv_URTI)
broom::tidy(model2_inv_URTI)
```

Add Interaction term

```{r}
model2_inv_URTI_Int <- lm((1/URTI)~(Med_temp+Med_temp_rng)*Rain_11,
                           data=data_time_URTI)

gvlma::gvlma(model2_inv_URTI_Int)
broom::tidy(model2_inv_URTI_Int)
anova(model2_inv_URTI,model2_inv_URTI_Int)
broom::glance(model2_inv_URTI)
broom::glance(model2_inv_URTI_Int)
```

Median temperature is significant at an alpha level of 5%. There is an interaction effect too (Rain_11 on Med_temp) and Anova test shows adding the interaction terms improves the model fit (p value = 0.004838). This interaction model has a higher R2 value too.

r.squared(model2_inv_URTI) --> 0.053
r.squared(model2_inv_URTI_Int)--> 0.077
adj.r.squared(model2_inv_URTI) --> 0.0467
adj.r.squared(model2_inv_URTI_Int)--> 0.0660

Report the model summary and significance using export_summs() function

```{r}
jtools::export_summs(model2_inv_URTI,model2_inv_URTI_Int,
             error_format = "(p = {p.value})",
             model.names = c("Main Effects Only",
                             "Interaction Effects"),
             digits = 3)
```

Probing Interaction Effects

```{r, fig.height=7, fig.width=9}
#Run Spotlight analysis, using Johnson-Neyman Techniques
# Median Temperature

# interactions::sim_slopes(model2_inv_URTI_Int,
#            pred = Rain_11,
#            modx = Med_temp,
#            johnson_neyman = T) ##[27.28, 28.61]
# 
# #Run interaction_plot() by adding benchmark for regions of significance
# # Use plot B to see the actual URTI numbers instead of y inverse
# #  a. Median Temperature
# interactions::interact_plot(model2_inv_URTI_Int,
#             pred="Med_temp",
#             modx = "Rain_11",
#             modx.labels = c("less than 11cm of rain fall",
#                             "rain fall >=11cm"),
#             interval = T,
#             int.width = .95,
#             colors = c("deeppink",
#                        "darkgreen"),
#             vary.lty = T,
#             line.thickness = 1,
#             legend.main = "") +
# ggplot2:: geom_vline(xintercept = 27.28, col = "red", linetype = 3, size = 1)+
# labs(title = "Effect of Rain fall and Temperature on rise in URTI Cases in Singapore",
#      subtitle = "For lower weekly rainfall, the number of URTI cases\ndereases with rise in temperature",
#      x = "Weekly Median Temperature",
#      y = "1/Number of URTI Cases",
#      caption = "Source: MOH, NEA websites") +
# annotate("text",
#          x = 28,
#          y = 0.0003,
#          label = "The shaded areas denote 95% confidence intervals.\nThe vertical line marks the boundary\nbetween regions of significance and non-significance\nbased on alpha at 5%") +
# theme(legend.position = "top")
```

```{r, fig.height=7, fig.width=9}
##############################################
#This model model2_URTI is just for plotting purpose.
model2_URTI_Int <- lm(URTI~(Med_temp+Med_temp_rng)*Rain_11,
                           data=data_time_URTI)

#Run Spotlight analysis, using Johnson-Neyman Techniques
#  Median Temperature
# sim_slopes(model2_URTI_Int, 
#            pred = Rain_11,
#            modx = Med_temp, 
#            johnson_neyman = F) 

interactions::sim_slopes(model2_URTI_Int,
           pred = Rain_11,
           modx = Med_temp,
           johnson_neyman = T) #[27.24, 28.48]

interactions::interact_plot(model2_URTI_Int,
              pred="Med_temp",
              modx = "Rain_11",
              modx.labels = c("less than 11cm of rain fall",
                              "rain fall >=11cm"),
              interval = T,
              int.width = .95,
              colors = c("deeppink",
                         "darkgreen"),
              vary.lty = T,
              line.thickness = 1,
              legend.main = "") +
  ggplot2::geom_vline(xintercept = 27.24, col = "red", linetype = 3, size = 1)+
  ggplot2::geom_vline(xintercept = 28.48, col = "red", linetype = 3, size = 1)+
  labs(title = "Effect of Rain fall and Temperature on rise in URTI Cases in Singapore",
       subtitle = "For lower weekly rainfall, the number of URTI cases\ndereases with rise in temperature",
       x = "Weekly Median Temperature",
       y = "Number of URTI Cases",
       caption = "Source: MOH, NEA websites") +
  annotate("text", 
           x = 28, 
           y = 2000,
           label = "The shaded areas denote 95% confidence intervals.\nThe vertical line marks the boundary\nbetween regions of significance and non-significance\nbased on alpha at 5%") + 
  theme(legend.position = "top")
```



## Interpretation of the Results

Associations between number of DF and URTI in the community with Rainfall and Temperature
In the modeling analyses, all models for the 4 clinical conditions except URTI failed the assumption checks. Despite failing the checks, we continued to test several modeling for DF as we are interested in the disease. The interpretation below will be focusing on URTI and DF.

We hypothesized that there will be an increase in URTI cases when the air temperature is low as people are more susceptible to common cold during cold weather. We observed that the number of URTI cases increase when the air temperature is low with low rainfall but a decrease in cases during low temperature with high rainfall. One of the reasons for this phenomenon could be people may wear more layers when the rain is heavier to protect them from the rain and cold as heavy rainfall is likely to be associated with cold weather. 

However, it is interesting to note that there is an increase in number of cases for DF and URTI during high air temperature with high rainfall (>11mm). The increase in DF in warmer temperature is likely attributed to the optimal temperature for mosquito to develop, research has shown that mosquito replicates faster at higher temperature during incubation period (Liu et al., 2017). The increase in URTI under high temperature is interesting and worthy of further exploration especially in tropical countries, understanding the aetiology would be helpful in reducing the cases. A possible explanation suggested for this was indoor transmission in air-conditioned environments accounted for most of the transmission (Tang, Wong and Hon, 2010).

Dengue fever was observed to decrease in high rainfall and low temperature. We understand that precipitation is associated to increase incidence of dengue fever.  However, our results seemed to support a recent study that claimed that heavy rainfall could disrupt breeding habitat and mosquitoesâ€™ reproductive cycle by washing away by the rain (Benedum, Seidahmed, Eltahir, Markuzon, 2018). 



## Implications

> Prof. Roh's Note: "This is where you provide the significance of the findings. Unlike the other sections, where your goal is to describe the results that you found (**what the data told you**). This is where you chime in and proactively discuss the meaning of the results."

Importance of keeping warm, dry and maintain good hygiene even during low rainfall to prevent developing URTI.



## Limitations and Future Directions

> Prof. Roh's Note: "Acknowledging limitations is not where you just provide a laundry list of what is missing and what should have done. Please take the responsibility of your analyses and inform your readers to understand what the results tell and donâ€™t (or canâ€™t) tell. More importantly, this is the section where you technically begin your next data science project, by highlighting what would be informative down the line to shed further light on what you have found here."

We have identified some potential pitfalls for our project.

- Failure of assumption checks for the models could be due to several reasons:
  - Insufficient data points. To improve the quality of data, daily number of cases should be used. 
  - Clinical conditions defined were too generalised. For example, Dengue fever can be caused by different strains of dengue virus.
- Limitations in obtaining data to conduct more informative analyses. For example, patientâ€™s demographic data. 



## References

Liu, Z., Zhang, Z., Lai, Z., Zhou, T., Jia, Z., Gu, J., ... & Chen, X. G. (2017). Temperature increase enhances Aedes albopictus competence to transmit dengue virus. Frontiers in microbiology, 8, 2337.
Benedum, C. M., Seidahmed, O. M., Eltahir, E. A., & Markuzon, N. (2018). Statistical modeling of the effect of rainfall flushing on dengue transmission in Singapore. PLoS neglected tropical diseases, 12(12), e0006935.
Tang, J. W., Lai, F. Y. L., Wong, F., & Hon, K. L. E. (2010). Incidence of common respiratory viral infections related to climate factors in hospitalized children in Hong Kong. Epidemiology & Infection, 138(2), 226-235.

## Appendix

> Appendix

## Contribution Statement

> Prof. Roh's Note: "Please describe your individual contribution to the team's project (**in detail**)."

